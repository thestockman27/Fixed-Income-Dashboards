{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('C:/Users/dstoc/Documents/Python Scripts/Fixed Income Dashboards/reduced_data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "df2 = df.copy()\n",
    "\n",
    "# Assuming 'Date' is the name of the column containing date information\n",
    "df2['Date'] = pd.to_datetime(df2['Date'])\n",
    "df2.set_index('Date', inplace=True)\n",
    "\n",
    "# Define the features (independent variables) and target (dependent variable)\n",
    "features = list(df2)\n",
    "\n",
    "# Define the lag order (number of lags to create)\n",
    "lag_order = 6  # Example: Creating lagged values for the past n periods\n",
    "\n",
    "for feature in features:\n",
    "    for lag in range(1, lag_order+1):\n",
    "        df2[f'{feature}_lag{lag}'] = df2[feature].shift(lag)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# replace inf and -inf with na and  drops rows with missing values in the lagged variables\n",
    "df2 = df2[6:]\n",
    "df2 = df2.replace([np.inf, -np.inf], np.nan).dropna(axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the number of periods to lead the target variable (1 period in this case)\n",
    "lead_periods = 1\n",
    "\n",
    "# redefine the features (independent variables) and target (dependent variable)\n",
    "features = list(df2)\n",
    "\n",
    "# Create a new variable representing the 'lead' of the target\n",
    "df2['YIELD_weekly_percent_change_10 YR_lead'] = df2['YIELD_weekly_percent_change_10 YR'].shift(-lead_periods)\n",
    "\n",
    "# Drop rows with missing values in the features or target (created by the shift)\n",
    "df2 = df2.dropna(subset=features + ['YIELD_weekly_percent_change_10 YR_lead'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['YIELD_weekly_percent_change_10 YR'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()\n",
    "features = list(df3.select_dtypes(include=['float64', 'int64']).columns)[0:(len(df3.columns)-1)]\n",
    "X = df3[features]\n",
    "y = df3['YIELD_weekly_percent_change_10 YR_lead']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()\n",
    "features = list(df3.columns)[0:(len(df3.columns)-1)]\n",
    "X = df3[features]\n",
    "y = df3['YIELD_weekly_percent_change_10 YR_lead']\n",
    "\n",
    "column_name = 'YIELD_weekly_percent_change_10 YR_lead'\n",
    "\n",
    "if column_name.lower() in map(str.lower, X.columns):\n",
    "    print(f\"The DataFrame contains the column '{column_name}'.\")\n",
    "else:\n",
    "    print(f\"The DataFrame does not contain the target variable: '{column_name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def select_top_variables(df, target_variable, num_variables):\n",
    "    # Calculate the rank correlation (Spearman's rho) with the target variable\n",
    "    correlation_matrix = df.corr(method='spearman')\n",
    "    \n",
    "    # Print the entire correlation matrix for debugging\n",
    "    print(correlation_matrix)\n",
    "    \n",
    "    # Extract the correlation values for the target variable\n",
    "    target_correlations = correlation_matrix[target_variable]\n",
    "    \n",
    "    # Sort variables based on absolute correlation values\n",
    "    sorted_variables = target_correlations.abs().sort_values(ascending=False)\n",
    "    \n",
    "    # Select the top variables\n",
    "    selected_variables = sorted_variables.index[:num_variables]\n",
    "    \n",
    "    # Extract the correlation values for the selected variables\n",
    "    selected_correlations = target_correlations[selected_variables]\n",
    "\n",
    "    corr_data = pd.DataFrame()\n",
    "    corr_data['Variable'] = selected_variables\n",
    "    corr_data['Correlation'] = selected_correlations\n",
    "\n",
    "    return corr_data\n",
    "\n",
    "# Assuming df3 is your DataFrame and 'YIELD_weekly_percent_change_10 YR' is the target variable\n",
    "correlated_variables = select_top_variables(df3, 'YIELD_weekly_percent_change_10 YR_lead', 200)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def select_top_variables(df, target_variable, num_variables):\n",
    "    # Calculate the rank correlation (Spearman's rho) with the target variable\n",
    "    correlation_matrix = df.corrwith(df[target_variable], method='spearman')\n",
    "    \n",
    "    # Sort variables based on absolute correlation values\n",
    "    sorted_variables = correlation_matrix.abs().sort_values(ascending=False)\n",
    "    \n",
    "    # Select the top variables\n",
    "    selected_variables = sorted_variables.index[:num_variables]\n",
    "    \n",
    "    # Extract the correlation values for the selected variables\n",
    "    selected_correlations = correlation_matrix[selected_variables]\n",
    "    \n",
    "    # Create a DataFrame to store the results\n",
    "    corr_data = pd.DataFrame({\n",
    "        'Variable': selected_variables,\n",
    "        'Correlation': selected_correlations\n",
    "    })\n",
    "    \n",
    "    return corr_data\n",
    "\n",
    "# Assuming df3 is your DataFrame and 'YIELD_weekly_percent_change_10 YR' is the target variable\n",
    "correlated_variables = select_top_variables(df3, 'YIELD_weekly_percent_change_10 YR_lead', 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Variable\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# Assuming X is your DataFrame containing the selected variables\n",
    "vif_results = calculate_vif(df3[correlated_variables['Variable']])\n",
    "\n",
    "# Print the variables with high VIF\n",
    "low_vif_variables = vif_results[vif_results[\"VIF\"] < 5][:21]\n",
    "print(\"Variables with low VIF:\")\n",
    "print(low_vif_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features2 = [feature for feature in features if feature != \"YIELD_weekly_percent_change_10 YR_lead\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### OLD CODE PRIOR TO UTILIZING GRIDSEARCH ####\n",
    "\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "# from math import sqrt\n",
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "# import numpy as np\n",
    "\n",
    "# # Define the features (independent variables) and target (dependent variable)\n",
    "# df4 = df3.copy()\n",
    "# features = low_vif_variables[\"Variable\"]\n",
    "# features2 = [feature for feature in features if feature != \"YIELD_weekly_percent_change_10 YR_lead\"]\n",
    "# X = df4[features2]\n",
    "# y = df4['YIELD_weekly_percent_change_10 YR_lead']\n",
    "\n",
    "# # Scale features\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# # Initialize the Gradient Boosting Regressor\n",
    "# gbm = GradientBoostingRegressor(n_estimators=50, random_state=248, learning_rate=0.3, subsample=0.75, \n",
    "#                                 max_depth=3, min_samples_leaf=2)\n",
    "\n",
    "# # Define the initial training size (e.g., 70% of the data)\n",
    "# initial_train_size = int(0.7 * len(df4))\n",
    "\n",
    "# # Create lists to store the predictions for each fold\n",
    "# y_pred_train = []\n",
    "# y_pred_test = []\n",
    "\n",
    "# # Create a function for expanding rolling window one-step-ahead cross-validation\n",
    "# def expanding_rolling_cv(X, y, model, initial_train_size):\n",
    "#     rmse_scores = []\n",
    "\n",
    "#     for i in range(initial_train_size, len(X)):\n",
    "#         X_train = X.iloc[:i]\n",
    "#         y_train = y.iloc[:i]\n",
    "\n",
    "#         # Train the model on the training data\n",
    "#         model.fit(X_train, y_train)\n",
    "\n",
    "#         # Append the prediction to the appropriate list\n",
    "#         if i == initial_train_size:\n",
    "#              # Record predictions from the training period\n",
    "#             y_pred = model.predict(X.iloc[:i])\n",
    "#             y_pred_train.extend(y_pred[:])\n",
    "#         else:\n",
    "#              # Make one-step-ahead prediction for the next period\n",
    "#             y_pred = model.predict(X.iloc[i:i+1])\n",
    "#             y_pred_test.append(y_pred[0])\n",
    "\n",
    "#             # Extract the actual target value for the next period\n",
    "#             y_actual = y.iloc[i]\n",
    "\n",
    "#             # Calculate RMSE for this fold\n",
    "#             rmse = sqrt(mean_squared_error([y_actual], [y_pred]))\n",
    "#             rmse_scores.append(rmse)\n",
    "\n",
    "#     return rmse_scores\n",
    "\n",
    "# # Perform expanding rolling window cross-validation and get RMSE scores\n",
    "# rmse_scores = expanding_rolling_cv(X, y, gbm, initial_train_size)\n",
    "\n",
    "# # Print the RMSE scores for each fold\n",
    "# for i, rmse in enumerate(rmse_scores, start=1):\n",
    "#     print(f'Fold {i} RMSE: {rmse:.4f}')\n",
    "\n",
    "# # Calculate the mean RMSE across all folds\n",
    "# mean_rmse = np.mean(rmse_scores)\n",
    "# print(f'Mean RMSE: {mean_rmse:.4f}')\n",
    "\n",
    "# # Use the predictions made during cross-validation as predictions of y\n",
    "# y_pred_cv = y_pred_train + y_pred_test\n",
    "\n",
    "# # Split the predictions into training and testing periods (for plotting)\n",
    "# y_train_pred = y_pred_cv[:initial_train_size]\n",
    "# y_test_pred = y_pred_cv[initial_train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import r2_score\n",
    "# # Split the actual 'GDP_lead' values into training and testing periods\n",
    "# y_train_actual = y.iloc[:initial_train_size]\n",
    "# y_test_actual = y.iloc[initial_train_size+1:]\n",
    "\n",
    "# # Create a time index for plotting\n",
    "# time_index_train = range(len(y_train_actual))\n",
    "# time_index_test = range(len(y_train_actual), len(y_train_actual) + len(y_test_actual))\n",
    "\n",
    "# # Calculate evaluation metrics for the training and testing periods\n",
    "# mae_train = mean_absolute_error(y_train_actual, y_train_pred)\n",
    "# rmse_train = sqrt(mean_squared_error(y_train_actual, y_train_pred))\n",
    "# r2_train = r2_score(y_train_actual, y_train_pred)\n",
    "# print(f'IS Mean Absolute Error (MAE): {mae_train:.4f}')\n",
    "# print(f'IS Root Mean Squared Error (RMSE): {rmse_train:.4f}')\n",
    "# print(f'IS R-squared (R2): {r2_train:.4f}')\n",
    "\n",
    "# mae_test = mean_absolute_error(y_test_actual, y_test_pred)\n",
    "# rmse_test = sqrt(mean_squared_error(y_test_actual, y_test_pred))\n",
    "# r2_test = r2_score(y_test_actual, y_test_pred)\n",
    "# print(f'OOS Mean Absolute Error (MAE): {mae_test:.4f}')\n",
    "# print(f'OOS Root Mean Squared Error (RMSE): {rmse_test:.4f}')\n",
    "# print(f'OOS R-squared (R2): {r2_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the actual vs. predicted values for the training and testing periods\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.subplot(2, 1, 1)\n",
    "# plt.plot(time_index_train, y_train_actual, label='Actual (Training)', color='blue')\n",
    "# plt.plot(time_index_train, y_train_pred, label='Predicted (Training)', linestyle='--', color='red')\n",
    "# plt.title('Actual vs. Predicted TARGET_lead (Training Period)')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.plot(time_index_test, y_test_actual, label='Actual (Testing)', color='blue')\n",
    "# plt.plot(time_index_test, y_test_pred, label='Predicted (Testing)', linestyle='--', color='red')\n",
    "# plt.title('Actual vs. Predicted TARGET_lead (Testing Period)')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "# df5 = df3.copy()\n",
    "# features = low_vif_variables[\"Variable\"]\n",
    "# features2 = [feature for feature in features if feature != \"YIELD_weekly_percent_change_10 YR_lead\"]\n",
    "# df5['YIELD_weekly_percent_change_10 YR_lead'] = np.where(df5['YIELD_weekly_percent_change_10 YR_lead'] < 0, 'LONG', 'SHORT')\n",
    "# X = df5[features2]\n",
    "# y = df5['YIELD_weekly_percent_change_10 YR_lead']\n",
    "# # Initialize the Gradient Boosting Classifier\n",
    "# gbm = GradientBoostingClassifier(n_estimators=100, random_state=248, learning_rate=0.01, subsample=0.75, max_depth=8, min_samples_leaf=5)\n",
    "\n",
    "# # Define the initial training size (e.g., 70% of the data)\n",
    "# initial_train_size = int(0.7 * len(df4))\n",
    "\n",
    "\n",
    "\n",
    "# # Create a function for expanding rolling window one-step-ahead cross-validation\n",
    "# def expanding_rolling_cv(X, y, model, initial_train_size):\n",
    "#     # Create lists to store the predictions for each fold\n",
    "#     y_pred_train = []\n",
    "#     y_pred_test = []\n",
    "\n",
    "#     for i in range(initial_train_size, len(X)):\n",
    "#         X_train = X.iloc[:i]\n",
    "#         y_train = y.iloc[:i]\n",
    "\n",
    "#         # Train the model on the training data\n",
    "#         model.fit(X_train, y_train)\n",
    "\n",
    "#         # Append the prediction to the appropriate list\n",
    "#         if i == initial_train_size:\n",
    "#             # Record predictions from training period\n",
    "#             y_pred = model.predict(X.iloc[:i])\n",
    "#             y_pred_train.extend(y_pred)\n",
    "            \n",
    "#         else:\n",
    "#             # Make one-step-ahead prediction for the next period\n",
    "#             y_pred = model.predict(X.iloc[i:i+1])\n",
    "#             y_pred_test.append(y_pred[0])\n",
    "\n",
    "#     return y_pred_train, y_pred_test\n",
    "\n",
    "# # Perform expanding rolling window cross-validation and get in-sample and out-of-sample predictions\n",
    "# y_pred_train, y_pred_test = expanding_rolling_cv(X, y, gbm, initial_train_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# conf_matrix_in_sample = confusion_matrix(y[:initial_train_size], y_pred_train, labels=['LONG', 'SHORT'])\n",
    "# conf_matrix_out_of_sample = confusion_matrix(y[(initial_train_size+1):], y_pred_test, labels=['LONG', 'SHORT'])\n",
    "\n",
    "# print(f'\\nConfusion Matrix for the In-Sample Period: \\n{conf_matrix_in_sample}')\n",
    "# print(f'Confusion Matrix for the Out-of-Sample Period: \\n{conf_matrix_out_of_sample}')\n",
    "\n",
    "# accuracy_in_sample = accuracy_score(y[:initial_train_size], y_pred_train)\n",
    "# accuracy_out_of_sample = accuracy_score(y[(initial_train_size+1):], y_pred_test)\n",
    "\n",
    "# print(f'\\nAccuracy for the In-Sample Period: {accuracy_in_sample:.4f}')\n",
    "# print(f'Accuracy for the Out-of-Sample Period: {accuracy_out_of_sample:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# Counter(y[(initial_train_size+1):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "# from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# # Assuming df3 is your DataFrame\n",
    "# df6 = df3.copy()\n",
    "\n",
    "# # Define your features and target variable\n",
    "# features = low_vif_variables[\"Variable\"]\n",
    "# features2 = [feature for feature in features if feature != \"YIELD_weekly_percent_change_10 YR_lead\"]\n",
    "# df6['YIELD_weekly_percent_change_10 YR_lead'] = np.where(df6['YIELD_weekly_percent_change_10 YR_lead'] < 0, 'LONG', 'SHORT')\n",
    "# X = df6[features2]\n",
    "# y = df6['YIELD_weekly_percent_change_10 YR_lead']\n",
    "\n",
    "# # Set up the parameter grid for grid search\n",
    "# param_grid = {\n",
    "#     'n_estimators': [50, 100, 150],\n",
    "#     'learning_rate': [0.01, 0.1, 0.3],\n",
    "#     'subsample': [0.75, 1],\n",
    "#     'max_depth': [3, 8, 13],\n",
    "#     'min_samples_leaf': [1, 3, 5]\n",
    "# }\n",
    "\n",
    "# # Initialize the Gradient Boosting Classifier\n",
    "# gbm = GradientBoostingClassifier(random_state=248)\n",
    "\n",
    "# # Define the initial training size (e.g., 70% of the data)\n",
    "# initial_train_size = int(0.7 * len(df6))\n",
    "\n",
    "# # Create the time series split for cross-validation\n",
    "# tscv = TimeSeriesSplit(n_splits=len(df6) - initial_train_size, test_size=1,gap=0, max_train_size=None)\n",
    "\n",
    "# # Perform grid search with cross-validation\n",
    "# grid_search = GridSearchCV(gbm, param_grid, cv=tscv, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "# grid_search.fit(X, y)\n",
    "\n",
    "# # Get the best model from the grid search\n",
    "# best_gbm = grid_search.best_estimator_\n",
    "\n",
    "# # Print the best hyperparameters\n",
    "# print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "# print(f'Accuracy of the Best Hyperparameters: {grid_search.best_score_:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch performed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, make_scorer\n",
    "\n",
    "# Assuming df4 is your DataFrame\n",
    "df6 = df3.copy()\n",
    "\n",
    "# Define your features and target variable\n",
    "features = low_vif_variables[\"Variable\"]\n",
    "features2 = [feature for feature in features if feature != \"YIELD_weekly_percent_change_10 YR_lead\"]\n",
    "df6['YIELD_weekly_percent_change_10 YR_lead'] = np.where(df6['YIELD_weekly_percent_change_10 YR_lead'] < 0, 'LONG', 'SHORT')\n",
    "X = df6[features2]\n",
    "y = df6['YIELD_weekly_percent_change_10 YR_lead']\n",
    "\n",
    "# Set up the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.75, 1],\n",
    "    'max_depth': [3, 8, 13],\n",
    "    'min_samples_leaf': [1, 5]\n",
    "}\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "gbm = GradientBoostingClassifier(random_state=248)\n",
    "\n",
    "# Define the initial training size (e.g., 70% of the data)\n",
    "initial_train_size = int(0.7 * len(df6))\n",
    "\n",
    "ys = []\n",
    "\n",
    "def Acc_Score(y_true,y_pred):\n",
    "    global ys\n",
    "    ys.append(y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)    \n",
    "    return acc\n",
    "\n",
    "def scorer():\n",
    "    return make_scorer(Acc_Score, greater_is_better=True)\n",
    "\n",
    "# Create the time series split for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=len(df6) - initial_train_size, test_size=1,gap=0, max_train_size=None)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(gbm, param_grid, cv=tscv, scoring=scorer(), verbose=1)#, n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_gbm = grid_search.best_estimator_\n",
    "\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(f'Accuracy of the Best Hyperparameters: {grid_search.best_score_:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "#Calculate and display accuracy\n",
    "accuracy_score(y[(initial_train_size):], ys[(((len(df6) - initial_train_size) * (grid_search.best_index_))):(len(df6) - initial_train_size) * (grid_search.best_index_ +1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# save the model to disk\n",
    "filename = '10Y_UST_gridsearch_results.sav'\n",
    "joblib.dump(grid_search, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estim_preds = ys[(((len(df6) - initial_train_size) * (grid_search.best_index_))):(len(df6) - initial_train_size) * (grid_search.best_index_ +1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estim_preds = np.array(best_estim_preds).flatten().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_estim_preds,'10Y_UST_best_estimator_predictions.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class backtest_object:\n",
    "  def __init__(self,grid_search, X, y, best_estim_preds, final_model):\n",
    "    self.grid_search = grid_search\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.best_estim_preds = best_estim_preds\n",
    "    self.final_model = best_gbm\n",
    "\n",
    "backtest_object_10Y_UST_v1 = backtest_object(grid_search, X, y, best_estim_preds, best_gbm)\n",
    "print(backtest_object_10Y_UST_v1)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Serialize the object using pickle and save to a file\n",
    "with open('backtest_object_10Y_UST_v2.pkl', 'wb') as file:\n",
    "    pickle.dump(backtest_object_10Y_UST_v2, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code for predicting off of new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_.predict(X.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_.predict_proba(X.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/modeling-pipeline-optimization-with-scikit-learn/\n",
    "import seaborn as sns \n",
    "sns.relplot(data=cv_results,\n",
    " kind='line',\n",
    " x='param_subsample',\n",
    " y='mean_test_score',\n",
    " hue='param_learning_rate',\n",
    " col='param_max_depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(cv_results['mean_test_score'])\n",
    "plt.xlabel('% Accuracy')\n",
    "plt.ylabel('Count')\n",
    " \n",
    "plt.title('Distribution of Out-of-Sample Accuracy Scores of Hyperparameter Combos\\n\\n',\n",
    "          fontweight = \"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "group1 = cv_results.query('param_learning_rate == 0.3')\n",
    "group2 = cv_results.query('param_learning_rate == 0.1')\n",
    "\n",
    "plt.hist(group1['mean_test_score'], label='LR = 0.3', alpha=0.75)\n",
    "plt.hist(group2['mean_test_score'], label='LR = 0.1', alpha=0.75)\n",
    "plt.xlabel('% Accuracy')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Distribution of Out-of-Sample Accuracy Scores of Hyperparameter Combos\\n\\n',\n",
    "          fontweight = \"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "group1 = cv_results.query('param_max_depth == 8')\n",
    "group2 = cv_results.query('param_max_depth == 3')\n",
    "\n",
    "plt.hist(group1['mean_test_score'], label='Max Depth = 8', alpha=0.75)\n",
    "plt.hist(group2['mean_test_score'], label='Max Depth = 3', alpha=0.75)\n",
    "plt.xlabel('% Accuracy')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Distribution of Out-of-Sample Accuracy Scores of Hyperparameter Combos\\n\\n',\n",
    "          fontweight = \"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "group1 = cv_results.query('param_min_samples_leaf == 5')\n",
    "group2 = cv_results.query('param_min_samples_leaf == 1')\n",
    "\n",
    "plt.hist(group1['mean_test_score'], label='Min Samples Leaf = 8', alpha=0.75)\n",
    "plt.hist(group2['mean_test_score'], label='Min Samples Leaf = 1', alpha=0.75)\n",
    "plt.xlabel('% Accuracy')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Distribution of Out-of-Sample Accuracy Scores of Hyperparameter Combos\\n\\n',\n",
    "          fontweight = \"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "group1 = cv_results.query('param_n_estimators == 50')\n",
    "group2 = cv_results.query('param_n_estimators == 100')\n",
    "\n",
    "plt.hist(group1['mean_test_score'], label='n_estimators = 50', alpha=0.75)\n",
    "plt.hist(group2['mean_test_score'], label='n_estimators = 100', alpha=0.75)\n",
    "plt.xlabel('% Accuracy')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Distribution of Out-of-Sample Accuracy Scores of Hyperparameter Combos\\n\\n',\n",
    "          fontweight = \"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "group1 = cv_results.query('param_subsample == 1')\n",
    "group2 = cv_results.query('param_subsample == .75')\n",
    "\n",
    "plt.hist(group1['mean_test_score'], label='subsample = 1.0', alpha=0.75)\n",
    "plt.hist(group2['mean_test_score'], label='subsample = 0.75', alpha=0.75)\n",
    "plt.xlabel('% Accuracy')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Distribution of Out-of-Sample Accuracy Scores of Hyperparameter Combos\\n\\n',\n",
    "          fontweight = \"bold\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
