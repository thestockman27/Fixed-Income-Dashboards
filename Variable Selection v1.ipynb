{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "df = pd.read_csv('C:/Users/dstoc/Documents/Python Scripts/Fixed Income Dashboards/reduced_data_cleaned.csv')\n",
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'Date' is the name of the column containing date information\n",
    "df2['Date'] = pd.to_datetime(df2['Date'])\n",
    "df2.set_index('Date', inplace=True)\n",
    "df2 = df2.replace([np.inf, -np.inf], np.nan).dropna(axis = 1)\n",
    "\n",
    "# Define the features (independent variables) and target (dependent variable)\n",
    "features = list(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lag order (number of lags to create)\n",
    "lag_order = 6  # Example: Creating lagged values for the past n periods\n",
    "# Define the initial training size (e.g., 70% of the data)\n",
    "initial_train_size = int(0.7 * len(df2)) - lag_order\n",
    "InSamp_length = len(df2) - initial_train_size - lag_order\n",
    "\n",
    "# Define the number of periods to lead the target variable (1 period in this case)\n",
    "lead_periods = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine the features (independent variables) and target (dependent variable)\n",
    "features = list(df2)\n",
    "\n",
    "# Create a new variable representing the 'lead' of the target\n",
    "df2['YIELD_weekly_percent_change_10 YR_lead'] = df2['YIELD_weekly_percent_change_10 YR'].shift(-lead_periods)\n",
    "\n",
    "# Drop rows with missing values in the features or target (created by the shift)\n",
    "df2 = df2.dropna(subset=features + ['YIELD_weekly_percent_change_10 YR_lead'])\n",
    "df3 = df2.copy()\n",
    "features = list(df3.columns)[0:(len(df3.columns)-1)]\n",
    "X = df3[features]\n",
    "y = df3['YIELD_weekly_percent_change_10 YR_lead']\n",
    "\n",
    "target_var = 'YIELD_weekly_percent_change_10 YR_lead'\n",
    "\n",
    "if target_var.lower() in map(str.lower, X.columns):\n",
    "    print(f\"The DataFrame contains the column '{target_var}'.\")\n",
    "else:\n",
    "    print(f\"The DataFrame X does not contain the target variable: '{target_var}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spearman's rank correlation is often used when the variables being compared may not have a linear relationship or when the assumptions of parametric correlation measures like Pearson's correlation are not met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.iloc[:InSamp_length,].tail()\n",
    "import pandas as pd\n",
    "\n",
    "def select_top_variables(df, target_variable, num_variables):\n",
    "    # Calculate the rank correlation (Spearman's rho) with the target variable\n",
    "    correlation_matrix = df.corrwith(df[target_variable], method='spearman')\n",
    "    \n",
    "    # Sort variables based on absolute correlation values\n",
    "    sorted_variables = correlation_matrix.abs().sort_values(ascending=False)\n",
    "    \n",
    "    # Select the top variables\n",
    "    selected_variables = sorted_variables.index[:num_variables]\n",
    "    \n",
    "    # Extract the correlation values for the selected variables\n",
    "    selected_correlations = correlation_matrix[selected_variables]\n",
    "    \n",
    "    # Create a DataFrame to store the results\n",
    "    corr_data = pd.DataFrame({\n",
    "        'Variable': selected_variables,\n",
    "        'Correlation': selected_correlations\n",
    "    })\n",
    "    \n",
    "    return corr_data\n",
    "\n",
    "# Assuming df3 is your DataFrame and 'YIELD_weekly_percent_change_10 YR' is the target variable\n",
    "correlated_variables = select_top_variables(df3.loc[df3.index[:InSamp_length]], 'YIELD_weekly_percent_change_10 YR_lead', 200)\n",
    "\n",
    "df3.loc[df3.index[:InSamp_length], correlated_variables['Variable']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variance Inflation Factor (VIF) is a tool used in multiple regression analysis to measure the degree of multicollinearity. Multicollinearity occurs when there is a correlation between multiple independent variables in a multiple regression model, which can adversely affect the regression results. The VIF estimates how much the variance of a regression coefficient is inflated due to multicollinearity. The dependent variable is the outcome being acted upon by the independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Variable\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# Assuming X is your DataFrame containing the selected variables\n",
    "#vif_results = calculate_vif(df3.iloc[:InSamp_length,[correlated_variables['Variable']]])\n",
    "vif_results = calculate_vif(df3.loc[df3.index[:InSamp_length], correlated_variables['Variable']])\n",
    "\n",
    "# Print the variables with high VIF\n",
    "low_vif_variables = vif_results.sort_values('VIF')[:51]\n",
    "print(\"Variables with low VIF:\")\n",
    "print(low_vif_variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save a pkl object that contains everything needed for the backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_variables = low_vif_variables.copy()\n",
    "class selected_variables_object:\n",
    "  def __init__(self,selected_variables, target_var, y, best_estim_preds, final_model):\n",
    "    self.selected_variables = selected_variables\n",
    "    self.target_var = target_var\n",
    "    self.y = y\n",
    "    self.best_estim_preds = best_estim_preds\n",
    "    self.final_model = best_gbm\n",
    "\n",
    "backtest_object_10Y_UST_v1_1 = backtest_object(grid_search, X, y, best_estim_preds, best_gbm)\n",
    "print(backtest_object_10Y_UST_v1_1)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Serialize the object using pickle and save to a file\n",
    "with open('backtest_object_10Y_UST_v1_1.pkl', 'wb') as file:\n",
    "    pickle.dump(backtest_object_10Y_UST_v1_1, file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
