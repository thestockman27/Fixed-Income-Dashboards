{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('C:/Users/dstoc/Documents/Python Scripts/Fixed Income Dashboards/reduced_data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "df2 = df.copy()\n",
    "\n",
    "# Assuming 'Date' is the name of the column containing date information\n",
    "df2['Date'] = pd.to_datetime(df2['Date'])\n",
    "df2.set_index('Date', inplace=True)\n",
    "df2 = df2.replace([np.inf, -np.inf], np.nan).dropna(axis = 1)\n",
    "\n",
    "# Define the features (independent variables) and target (dependent variable)\n",
    "features = list(df2)\n",
    "# Define the lag order (number of lags to create)\n",
    "lag_order = 6  # Example: Creating lagged values for the past n periods\n",
    "# Define the initial training size (e.g., 70% of the data)\n",
    "initial_train_size = int(0.7 * len(df2)) - lag_order\n",
    "InSamp_length = len(df2) - initial_train_size - lag_order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the number of periods to lead the target variable (1 period in this case)\n",
    "lead_periods = 1\n",
    "\n",
    "# redefine the features (independent variables) and target (dependent variable)\n",
    "features = list(df2)\n",
    "\n",
    "# Create a new variable representing the 'lead' of the target\n",
    "df2['YIELD_weekly_percent_change_10 YR_lead'] = df2['YIELD_weekly_percent_change_10 YR'].shift(-lead_periods)\n",
    "\n",
    "# Drop rows with missing values in the features or target (created by the shift)\n",
    "df2 = df2.dropna(subset=features + ['YIELD_weekly_percent_change_10 YR_lead'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['YIELD_weekly_percent_change_10 YR'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()\n",
    "features = list(df3.select_dtypes(include=['float64', 'int64']).columns)[0:(len(df3.columns)-1)]\n",
    "X = df3[features]\n",
    "y = df3['YIELD_weekly_percent_change_10 YR_lead']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.copy()\n",
    "features = list(df3.columns)[0:(len(df3.columns)-1)]\n",
    "X = df3[features]\n",
    "y = df3['YIELD_weekly_percent_change_10 YR_lead']\n",
    "\n",
    "target_var = 'YIELD_weekly_percent_change_10 YR_lead'\n",
    "\n",
    "if target_var.lower() in map(str.lower, X.columns):\n",
    "    print(f\"The DataFrame contains the column '{target_var}'.\")\n",
    "else:\n",
    "    print(f\"The DataFrame X does not contain the target variable: '{target_var}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spearman's rank correlation is often used when the variables being compared may not have a linear relationship or when the assumptions of parametric correlation measures like Pearson's correlation are not met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.iloc[:InSamp_length,].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def select_top_variables(df, target_variable, num_variables):\n",
    "    # Calculate the rank correlation (Spearman's rho) with the target variable\n",
    "    correlation_matrix = df.corrwith(df[target_variable], method='spearman')\n",
    "    \n",
    "    # Sort variables based on absolute correlation values\n",
    "    sorted_variables = correlation_matrix.abs().sort_values(ascending=False)\n",
    "    \n",
    "    # Select the top variables\n",
    "    selected_variables = sorted_variables.index[:num_variables]\n",
    "    \n",
    "    # Extract the correlation values for the selected variables\n",
    "    selected_correlations = correlation_matrix[selected_variables]\n",
    "    \n",
    "    # Create a DataFrame to store the results\n",
    "    corr_data = pd.DataFrame({\n",
    "        'Variable': selected_variables,\n",
    "        'Correlation': selected_correlations\n",
    "    })\n",
    "    \n",
    "    return corr_data\n",
    "\n",
    "# Assuming df3 is your DataFrame and 'YIELD_weekly_percent_change_10 YR' is the target variable\n",
    "correlated_variables = select_top_variables(df3.loc[df3.index[:InSamp_length]], 'YIELD_weekly_percent_change_10 YR_lead', 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.loc[df3.index[:InSamp_length], correlated_variables['Variable']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calculate_vif(X):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Variable\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return vif_data\n",
    "\n",
    "# Assuming X is your DataFrame containing the selected variables\n",
    "#vif_results = calculate_vif(df3.iloc[:InSamp_length,[correlated_variables['Variable']]])\n",
    "vif_results = calculate_vif(df3.loc[df3.index[:InSamp_length], correlated_variables['Variable']])\n",
    "\n",
    "# Print the variables with high VIF\n",
    "low_vif_variables = vif_results.sort_values('VIF')[:51]\n",
    "print(\"Variables with low VIF:\")\n",
    "print(low_vif_variables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, make_scorer\n",
    "\n",
    "# Assuming df4 is your DataFrame\n",
    "df6 = df3.copy()\n",
    "df6 = df6[low_vif_variables[\"Variable\"]]\n",
    "# Define your features and target variable\n",
    "features = low_vif_variables[\"Variable\"]\n",
    "features2 = [feature for feature in features if feature != target_var]\n",
    "\n",
    "\n",
    "for feature in features:\n",
    "    for lag in range(1, lag_order+1):\n",
    "        df6[f'{feature}_lag{lag}'] = df6[feature].shift(lag)\n",
    "\n",
    "\n",
    "# replace inf and -inf with na and  drops rows with missing values in the lagged variables\n",
    "df6 = df6[6:]\n",
    "df6 = df6.replace([np.inf, -np.inf], np.nan).dropna(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6[target_var] = np.where(df6[target_var] < 0, 'LONG', 'SHORT')\n",
    "X = df6[features2]\n",
    "y = df6[target_var]\n",
    "\n",
    "if target_var.lower() in map(str.lower, X.columns):\n",
    "    print(f\"The DataFrame contains the column '{target_var}'. ABORT BACK-TEST\")\n",
    "else:\n",
    "    print(f\"The DataFrame X does not contain the target variable: '{target_var}'. You may proceed with the back-test.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                                    \n",
    "                                                                    ## Gridsearch performed below ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameter grid for grid search\n",
    "# param_grid = {\n",
    "#     'n_estimators': [100, 150],\n",
    "#     'learning_rate': [0.01, 0.1],\n",
    "#     'subsample': [0.75],\n",
    "#     'max_depth': [3, 13],\n",
    "#     'min_samples_leaf': [2, 5]\n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50],\n",
    "    'learning_rate': [0.3],\n",
    "    'subsample': [0.75],\n",
    "    'max_depth': [5,15],\n",
    "    'min_samples_leaf': [5]\n",
    "}\n",
    "\n",
    "# Initialize the Gradient Boosting Classifier\n",
    "gbm = GradientBoostingClassifier(random_state=248)\n",
    "\n",
    "\n",
    "ys = []\n",
    "\n",
    "def Acc_Score(y_true,y_pred):\n",
    "    global ys\n",
    "    ys.append(y_pred)\n",
    "    acc = accuracy_score(y_true, y_pred)    \n",
    "    return acc\n",
    "\n",
    "def scorer():\n",
    "    return make_scorer(Acc_Score, greater_is_better=True)\n",
    "\n",
    "# Create the time series split for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=len(df6) - initial_train_size, test_size=1,gap=0, max_train_size=None)\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(gbm, param_grid, cv=tscv, scoring=scorer(), verbose=1)#, n_jobs=-1)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_gbm = grid_search.best_estimator_\n",
    "\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(f'Accuracy of the Best Hyperparameters: {grid_search.best_score_:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "#Calculate and display accuracy\n",
    "accuracy_score(y[(initial_train_size):], ys[(((len(df6) - initial_train_size) * (grid_search.best_index_))):(len(df6) - initial_train_size) * (grid_search.best_index_ +1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "# save the model to disk\n",
    "filename = '10Y_UST_gridsearch_results.sav'\n",
    "joblib.dump(grid_search, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estim_preds = ys[(((len(df6) - initial_train_size) * (grid_search.best_index_))):(len(df6) - initial_train_size) * (grid_search.best_index_ +1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_estim_preds = np.array(best_estim_preds).flatten().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(best_estim_preds,'10Y_UST_best_estimator_predictions.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class backtest_object:\n",
    "  def __init__(self,grid_search, X, y, best_estim_preds, final_model):\n",
    "    self.grid_search = grid_search\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.best_estim_preds = best_estim_preds\n",
    "    self.final_model = best_gbm\n",
    "\n",
    "backtest_object_10Y_UST_v1_1 = backtest_object(grid_search, X, y, best_estim_preds, best_gbm)\n",
    "print(backtest_object_10Y_UST_v1_1)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Serialize the object using pickle and save to a file\n",
    "with open('backtest_object_10Y_UST_v1_1.pkl', 'wb') as file:\n",
    "    pickle.dump(backtest_object_10Y_UST_v1_1, file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code for predicting off of new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_.predict(X.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_.predict_proba(X.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/modeling-pipeline-optimization-with-scikit-learn/\n",
    "import seaborn as sns \n",
    "sns.relplot(data=cv_results,\n",
    " kind='line',\n",
    " x='param_subsample',\n",
    " y='mean_test_score',\n",
    " hue='param_learning_rate',\n",
    " col='param_max_depth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(cv_results['mean_test_score'])\n",
    "plt.xlabel('% Accuracy')\n",
    "plt.ylabel('Count')\n",
    " \n",
    "plt.title('Distribution of Out-of-Sample Accuracy Scores of Hyperparameter Combos\\n\\n',\n",
    "          fontweight = \"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "group1 = cv_results.query('param_learning_rate == 0.3')\n",
    "group2 = cv_results.query('param_learning_rate == 0.1')\n",
    "\n",
    "plt.hist(group1['mean_test_score'], label='LR = 0.3', alpha=0.75)\n",
    "plt.hist(group2['mean_test_score'], label='LR = 0.1', alpha=0.75)\n",
    "plt.xlabel('% Accuracy')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Distribution of Out-of-Sample Accuracy Scores of Hyperparameter Combos\\n\\n',\n",
    "          fontweight = \"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "group1 = cv_results.query('param_max_depth == 15')\n",
    "group2 = cv_results.query('param_max_depth == 5')\n",
    "\n",
    "plt.hist(group1['mean_test_score'], label='Max Depth = 15', alpha=0.75)\n",
    "plt.hist(group2['mean_test_score'], label='Max Depth = 5', alpha=0.75)\n",
    "plt.xlabel('% Accuracy')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Distribution of Out-of-Sample Accuracy Scores of Hyperparameter Combos\\n\\n',\n",
    "          fontweight = \"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "group1 = cv_results.query('param_min_samples_leaf == 5')\n",
    "group2 = cv_results.query('param_min_samples_leaf == 1')\n",
    "\n",
    "plt.hist(group1['mean_test_score'], label='Min Samples Leaf = 8', alpha=0.75)\n",
    "plt.hist(group2['mean_test_score'], label='Min Samples Leaf = 1', alpha=0.75)\n",
    "plt.xlabel('% Accuracy')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Distribution of Out-of-Sample Accuracy Scores of Hyperparameter Combos\\n\\n',\n",
    "          fontweight = \"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "group1 = cv_results.query('param_n_estimators == 50')\n",
    "group2 = cv_results.query('param_n_estimators == 100')\n",
    "\n",
    "plt.hist(group1['mean_test_score'], label='n_estimators = 50', alpha=0.75)\n",
    "plt.hist(group2['mean_test_score'], label='n_estimators = 100', alpha=0.75)\n",
    "plt.xlabel('% Accuracy')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Distribution of Out-of-Sample Accuracy Scores of Hyperparameter Combos\\n\\n',\n",
    "          fontweight = \"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "group1 = cv_results.query('param_subsample == 1')\n",
    "group2 = cv_results.query('param_subsample == .75')\n",
    "\n",
    "plt.hist(group1['mean_test_score'], label='subsample = 1.0', alpha=0.75)\n",
    "plt.hist(group2['mean_test_score'], label='subsample = 0.75', alpha=0.75)\n",
    "plt.xlabel('% Accuracy')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Distribution of Out-of-Sample Accuracy Scores of Hyperparameter Combos\\n\\n',\n",
    "          fontweight = \"bold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv.split(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index,:]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.index[(len(df6) - initial_train_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot your own learning curve\n",
    "\n",
    "cv_perf = pd.DataFrame(cv_results.iloc[0,10:282])\n",
    "cv_perf = cv_perf.rename(columns={0: 'score'})\n",
    "cv_perf['Expanding_Average'] = cv_perf['score'].expanding().mean()\n",
    "#cv_perf.reindex(X.index[(len(df6) - initial_train_size):])\n",
    "# Plot the expanding average\n",
    "plt.figure(figsize=(10, 6))\n",
    "#plt.plot(cv_perf['Value'], label='Original Values', marker='o')\n",
    "plt.plot(cv_perf['Expanding_Average'], label='Expanding Average', linestyle='--', marker='o')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Cross Validation Scoring Plot')\n",
    "plt.legend()\n",
    "# Show only every 10th tick on the x-axis\n",
    "plt.xticks(cv_perf.index[::50])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import learning_curve, train_test_split\n",
    "\n",
    "# # Get the best estimator from the grid search\n",
    "# best_gbm = grid_search.best_estimator_\n",
    "\n",
    "# # Plot learning curve\n",
    "# train_sizes, train_scores, test_scores = learning_curve(best_gbm, X_train, y_train, cv=tscv, scoring=make_scorer(accuracy_score), train_sizes=np.ones((len(df6) - initial_train_size)))\n",
    "\n",
    "# # Calculate mean and standard deviation for training set scores and test set scores\n",
    "# train_mean = np.mean(train_scores, axis=1)\n",
    "# train_std = np.std(train_scores, axis=1)\n",
    "# test_mean = np.mean(test_scores, axis=1)\n",
    "# test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# # Plot the learning curve\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(train_sizes, train_mean, label='Training accuracy', color='blue', marker='o')\n",
    "# plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='blue', alpha=0.2)\n",
    "# plt.plot(train_sizes, test_mean, label='Cross-validation accuracy', color='red', marker='o')\n",
    "# plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color='red', alpha=0.2)\n",
    "\n",
    "# # Add labels and legend\n",
    "# plt.title('Learning Curve for GBM Classifier')\n",
    "# plt.xlabel('Number of training examples')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend(loc='best')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import learning_curve, train_test_split\n",
    "\n",
    "# # Get the best estimator from the grid search\n",
    "# best_gbm = grid_search.best_estimator_\n",
    "\n",
    "# # Plot learning curve\n",
    "# train_sizes, train_scores, test_scores = learning_curve(best_gbm, X_train, y_train, cv=tscv, scoring=make_scorer(accuracy_score), train_sizes=np.ones((len(df6) - initial_train_size)))\n",
    "\n",
    "# # Calculate mean and standard deviation for training set scores and test set scores\n",
    "# train_mean = np.mean(train_scores, axis=1)\n",
    "# train_std = np.std(train_scores, axis=1)\n",
    "# test_mean = np.mean(test_scores, axis=1)\n",
    "# test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# # Plot the learning curve\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(train_sizes, train_mean, label='Training accuracy', color='blue', marker='o')\n",
    "# plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, color='blue', alpha=0.2)\n",
    "# plt.plot(train_sizes, test_mean, label='Cross-validation accuracy', color='red', marker='o')\n",
    "# plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, color='red', alpha=0.2)\n",
    "\n",
    "# # Add labels and legend\n",
    "# plt.title('Learning Curve for GBM Classifier')\n",
    "# plt.xlabel('Number of training examples')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend(loc='best')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
